{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4456dec7-db7c-4ccd-9b0c-cab367823b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\oriel\\miniconda3\\envs\\cvision\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oriel\\miniconda3\\envs\\cvision\\lib\\site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\oriel\\miniconda3\\envs\\cvision\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 15.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.9/10.4 MB 19.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.1/10.4 MB 18.6 MB/s eta 0:00:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 102, in read\n",
      "    self.__buf.write(data)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\tempfile.py\", line 499, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 379, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "        reqs, check_supported_wheels=not options.target_dir\n",
      "    )\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 554, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        partially_downloaded_reqs,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        parallel_builds=parallel_builds,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 469, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 184, in __call__\n",
      "    for chunk in chunks:\n",
      "                 ^^^^^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 55, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "                 ^^^^^^^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "                 ~~~~~~~~~~~~~~~~~~~^\n",
      "        chunk_size,\n",
      "        ^^^^^^^^^^^\n",
      "    ...<22 lines>...\n",
      "        decode_content=False,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "         ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\contextlib.py\", line 162, in __exit__\n",
      "    self.gen.throw(value)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\Oriel\\miniconda3\\envs\\cvision\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: OSError(28, 'No space left on device')\", OSError(28, 'No space left on device'))\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5131e2c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetrImageProcessor, DetrForObjectDetection\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw, ImageFont\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e0341d4-15d7-4ceb-b106-e5565fe24c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class media_pipeline:\n",
    "    def __init__(self):\n",
    "        self.object_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "        self.object_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "\n",
    "        self.image = None\n",
    "        self.results = None\n",
    "\n",
    "    def upload_image(self, pil_img):\n",
    "        pil_img = pil_img.convert(\"RGB\")\n",
    "        width, height = pil_img.size\n",
    "        min_side = min(width, height)\n",
    "    \n",
    "        if min_side != 800:\n",
    "            scale = 800 / min_side\n",
    "            new_size = (round(width * scale), round(height * scale))\n",
    "            pil_img = pil_img.resize(new_size, Image.BICUBIC)\n",
    "    \n",
    "        self.image = pil_img\n",
    "\n",
    "    def detect_objects(self, threshold=0.9):\n",
    "        if self.image is None:\n",
    "            raise ValueError(\"No image uploaded.\")\n",
    "        \n",
    "        inputs = self.object_processor(images=self.image, return_tensors='pt')\n",
    "        outputs = self.object_model(**inputs)\n",
    "\n",
    "        target_sizes = torch.tensor([self.image.size[::-1]])\n",
    "        self.results = self.object_processor.post_process_object_detection(\n",
    "            outputs, target_sizes=target_sizes, threshold=threshold\n",
    "        )[0]\n",
    "\n",
    "    def draw_boxes(self, flipped=\"False\"):\n",
    "        if self.results is None:\n",
    "            raise ValueError(\"No detection results. Run detect_objects() first.\")\n",
    "    \n",
    "        self.image = self.image.convert(\"RGB\")\n",
    "        annotated_image = self.image.copy()\n",
    "        draw = ImageDraw.Draw(annotated_image)\n",
    "    \n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "        for score, label, box in zip(self.results[\"scores\"], self.results[\"labels\"], self.results[\"boxes\"]):\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            label_name = self.object_model.config.id2label[label.item()]\n",
    "            score_val = round(score.item(), 3)\n",
    "    \n",
    "            draw.rectangle(box, outline=\"red\", width=2)\n",
    "            draw.text((box[0] + 5, box[1] + 5), f\"{label_name}: {score_val}\", fill=\"white\", font=font)\n",
    "        if(flipped == \"True\"):\n",
    "            annotated_image = annotated_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return annotated_image\n",
    "\n",
    "    def get_object_scores(self):\n",
    "        if self.results is None:\n",
    "            raise ValueError(\"No detection results. Run detect_objects() first.\")\n",
    "        \n",
    "        width, height = self.image.size\n",
    "        image_area = width * height\n",
    "    \n",
    "        self.objects = []\n",
    "    \n",
    "        for score, label, box in zip(self.results[\"scores\"], self.results[\"labels\"], self.results[\"boxes\"]):\n",
    "            box = [round(coord, 0) for coord in box.tolist()]\n",
    "            label_name = self.object_model.config.id2label[label.item()]\n",
    "            score_val = round(score.item(), 4)\n",
    "            \n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            box_area = max(0, (x_max - x_min)) * max(0, (y_max - y_min))\n",
    "            area_ratio = round(box_area / image_area, 6)\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "    \n",
    "            self.objects.append({\n",
    "                \"label\": label_name,\n",
    "                \"score\": score_val,\n",
    "                \"box\": box,\n",
    "                \"width\":width,\n",
    "                \"height\":height,\n",
    "                \"area\": box_area,\n",
    "                \"area_ratio\": area_ratio\n",
    "            })\n",
    "        self.objects.sort(key=lambda x: x[\"area_ratio\"], reverse=True)\n",
    "\n",
    "        return self.objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27cc3a37-f4ee-460e-8267-32836609deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.patches as patches  \n",
    "from transformers import AutoProcessor, AutoModelForCausalLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "379ca721-cf33-44c7-8cc5-568f6531c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_pipeline:\n",
    "    def __init__(self):\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        \n",
    "        self.test_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\n",
    "        self.test_processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True)\n",
    "\n",
    "        self.image = None\n",
    "        self.results = None\n",
    "\n",
    "    def upload_image(self, pil_img):\n",
    "        pil_img = pil_img.convert(\"RGB\")    \n",
    "        self.image = pil_img\n",
    "\n",
    "    def detect_objects(self, task_prompt, text_input=None):\n",
    "        if text_input is None:\n",
    "            prompt = task_prompt\n",
    "        else:\n",
    "            prompt = task_prompt + text_input\n",
    "        inputs = self.test_processor(text=prompt, images=self.image, return_tensors=\"pt\")\n",
    "        generated_ids = self.test_model.generate(\n",
    "          input_ids=inputs[\"input_ids\"],\n",
    "          pixel_values=inputs[\"pixel_values\"],\n",
    "          max_new_tokens=1024,\n",
    "          early_stopping=False,\n",
    "          do_sample=False,\n",
    "          num_beams=3,\n",
    "        )\n",
    "        generated_text = self.test_processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "        self.object = self.test_processor.post_process_generation(\n",
    "            generated_text, \n",
    "            task=task_prompt, \n",
    "            image_size=self.image.size\n",
    "        )\n",
    "    \n",
    "        self.object\n",
    "\n",
    "    def collect_data(self, task_prompt, illustrate=False):\n",
    "        self.image = self.image.convert(\"RGB\")\n",
    "        objects = self.object[task_prompt]\n",
    "        uids = []\n",
    "\n",
    "        labels_list = []\n",
    "        for label in objects[\"labels\"]:\n",
    "            \n",
    "            val = labels_list.count(label)\n",
    "\n",
    "            labels_list.append(label)\n",
    "            uid = f\"uid_{label}_{val}\"\n",
    "            uids.append(uid)\n",
    "\n",
    "        area_ratio = []\n",
    "        for i, row in enumerate(objects[\"bboxes\"]):\n",
    "            x_min, y_min, x_max, y_max = row\n",
    "            width, height = self.image.size\n",
    "            area = ((x_max - x_min) * (y_max - y_min)) / (width * height)\n",
    "            area_ratio.append(round(area,4))\n",
    "            self.object[task_prompt][\"bboxes\"][i] = [round(item, 2) for item in row]\n",
    "\n",
    "    \n",
    "        self.object[task_prompt]['uids'] = uids\n",
    "        self.object[task_prompt]['area_ratio'] = area_ratio\n",
    "        print(\"Data Collected\")\n",
    "\n",
    "        if illustrate is True:\n",
    "            font = ImageFont.load_default()\n",
    "            annotated_image = self.image.copy()\n",
    "            draw = ImageDraw.Draw(annotated_image)\n",
    "        \n",
    "            for bbox, label, uid in zip(objects['bboxes'], objects['labels'], uids):\n",
    "                box = [round(coord, 2) for coord in bbox]\n",
    "                label_name = label\n",
    "                draw.rectangle(box, outline=\"red\", width=2)\n",
    "                draw.text((box[0] + 5, box[1] + 5), f\"{label_name} ({uid})\", fill=\"white\", font=font)\n",
    "        \n",
    "            annotated_image.show()\n",
    "        return self.object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "835ce7ee-e774-4539-8714-ddf998bb513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collected\n",
      "Data Collected\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "pipeline = test_pipeline()\n",
    "pipeline.upload_image(Image.open(\"1.JPG\"))\n",
    "pipeline.detect_objects('<OD>')\n",
    "image1data = pipeline.collect_data('<OD>', illustrate=False)\n",
    "\n",
    "pipeline.upload_image(Image.open(\"2.JPG\"))\n",
    "pipeline.detect_objects('<OD>')\n",
    "image2data = pipeline.collect_data('<OD>', illustrate=False)\n",
    "score = compare_images(image1data, image2data, \"<OD>\", \"labels\" ,\"uids\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "0d69633b-fc45-4199-8161-64064b06af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pictures = [f\"a{i}.jpg\" for i in range(7) if i != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "62343e66-ed8e-42ae-a949-4ac605c95b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1.jpg', 'a2.jpg', 'a3.jpg', 'a4.jpg', 'a5.jpg', 'a6.jpg']"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3490d787-e899-4f5d-8308-4c4eb5a7e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(image1, image2, task_prompt, col1, col2):\n",
    "    score = 0\n",
    "    used_rows = []\n",
    "    total = len(image1[task_prompt][col1])  # total number of items in image1\n",
    "    total2 = len(image2[task_prompt][col1])\n",
    "\n",
    "    for i in range(total2):\n",
    "        val1 = image2[task_prompt][col1][i]\n",
    "        uid1 = image2[task_prompt][col2][i]\n",
    "\n",
    "        for j, val2 in enumerate(image1[task_prompt][col1]):\n",
    "            # we can easily calculate more metrics here\n",
    "            # we need to determine how many columns are there, for each value that matches a point is added with a weight of how much its worth.\n",
    "            # ex: take how many rows matched up and then compare the area_ratios of matching rows to create another weight that we can multiply by\n",
    "            if j in used_rows:\n",
    "                continue\n",
    "            uid2 = image1[task_prompt][col2][j]\n",
    "\n",
    "            if val1 == val2:\n",
    "                score += 0.5\n",
    "                if uid1 == uid2:\n",
    "                    score += 0.5\n",
    "                used_rows.append(j)\n",
    "                break\n",
    "\n",
    "    return score / total if total > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48eb449-60c2-4650-b643-75e1f3da4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "pipeline = test_pipeline()\n",
    "cap = cv2.VideoCapture(\"IMG_6957.mp4\")\n",
    "\n",
    "frame_idx = 0\n",
    "\n",
    "ret, curr_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "curr_frame_pil = Image.fromarray(cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "while True:\n",
    "    ret, next_frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_idx += 1\n",
    "    if frame_idx % 30 != 0:\n",
    "        continue  # Skip until the 30th frame\n",
    "\n",
    "    next_frame_pil = Image.fromarray(cv2.cvtColor(next_frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    pipeline.upload_image(curr_frame_pil)\n",
    "    pipeline.detect_objects('<OD>')\n",
    "    image1data = pipeline.collect_data('<OD>', illustrate=True)\n",
    "    \n",
    "    pipeline.upload_image(next_frame_pil)\n",
    "    pipeline.detect_objects('<OD>')\n",
    "    image2data = pipeline.collect_data('<OD>', illustrate=True)\n",
    "    \n",
    "    score = compare_images(image1data, image2data, \"<OD>\", \"labels\", \"uids\")\n",
    "    print(f\"[Frame {frame_idx}] Similarity Score: {score:.2f}\")\n",
    "    \n",
    "    curr_frame_pil = next_frame_pil  # Update for next comparison\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "bfd15042-1571-442c-b6af-c4a9097578ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collected\n",
      "Data Collected\n",
      "['bottle', 'cabinetry', 'chair', 'human eye', 'human eye', 'human face', 'human mouth', 'human nose', 'person', 'power outlet', 'power plugs and sockets']\n",
      "['bed', 'box', 'cabinetry', 'door handle', 'human face', 'human hand', 'human hand', 'human mouth', 'human nose', 'person']\n",
      "['uid_bottle_0', 'uid_cabinetry_0', 'uid_chair_0', 'uid_human eye_0', 'uid_human eye_1', 'uid_human face_0', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0', 'uid_power outlet_0', 'uid_power plugs and sockets_0']\n",
      "['uid_bed_0', 'uid_box_0', 'uid_cabinetry_0', 'uid_door handle_0', 'uid_human face_0', 'uid_human hand_0', 'uid_human hand_1', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0']\n",
      "0.45454545454545453\n",
      "Data Collected\n",
      "Data Collected\n",
      "['bed', 'box', 'cabinetry', 'door handle', 'human face', 'human hand', 'human hand', 'human mouth', 'human nose', 'person']\n",
      "['box', 'human eye', 'human eye', 'human face', 'human mouth', 'human nose', 'person']\n",
      "['uid_bed_0', 'uid_box_0', 'uid_cabinetry_0', 'uid_door handle_0', 'uid_human face_0', 'uid_human hand_0', 'uid_human hand_1', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0']\n",
      "['uid_box_0', 'uid_human eye_0', 'uid_human eye_1', 'uid_human face_0', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0']\n",
      "0.5\n",
      "Data Collected\n",
      "Data Collected\n",
      "['box', 'human eye', 'human eye', 'human face', 'human mouth', 'human nose', 'person']\n",
      "['bathtub', 'coffee cup', 'human face', 'human mouth', 'human nose', 'person', 'power outlet']\n",
      "['uid_box_0', 'uid_human eye_0', 'uid_human eye_1', 'uid_human face_0', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0']\n",
      "['uid_bathtub_0', 'uid_coffee cup_0', 'uid_human face_0', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0', 'uid_power outlet_0']\n",
      "0.5714285714285714\n",
      "Data Collected\n",
      "Data Collected\n",
      "['bathtub', 'coffee cup', 'human face', 'human mouth', 'human nose', 'person', 'power outlet']\n",
      "['bed', 'door', 'human eye', 'human eye', 'human face', 'human mouth', 'human nose', 'person', 'power outlet']\n",
      "['uid_bathtub_0', 'uid_coffee cup_0', 'uid_human face_0', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0', 'uid_power outlet_0']\n",
      "['uid_bed_0', 'uid_door_0', 'uid_human eye_0', 'uid_human eye_1', 'uid_human face_0', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0', 'uid_power outlet_0']\n",
      "0.7142857142857143\n",
      "Data Collected\n",
      "Data Collected\n",
      "['bed', 'door', 'human eye', 'human eye', 'human face', 'human mouth', 'human nose', 'person', 'power outlet']\n",
      "['box', 'human eye', 'human eye', 'human face', 'human mouth', 'human nose', 'person']\n",
      "['uid_bed_0', 'uid_door_0', 'uid_human eye_0', 'uid_human eye_1', 'uid_human face_0', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0', 'uid_power outlet_0']\n",
      "['uid_box_0', 'uid_human eye_0', 'uid_human eye_1', 'uid_human face_0', 'uid_human mouth_0', 'uid_human nose_0', 'uid_person_0']\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "pipeline = test_pipeline()\n",
    "\n",
    "for i, picture in enumerate(pictures):\n",
    "    if i+1 < len(pictures):\n",
    "        pipeline.upload_image(Image.open(picture))\n",
    "        pipeline.detect_objects('<OD>')\n",
    "        image1data = pipeline.collect_data('<OD>', illustrate=False)\n",
    "        \n",
    "        pipeline.upload_image(Image.open(pictures[i+1]))\n",
    "        pipeline.detect_objects('<OD>')\n",
    "        image2data = pipeline.collect_data('<OD>', illustrate=False)\n",
    "\n",
    "        print(image1data['<OD>'][\"labels\"])\n",
    "        print(image2data['<OD>'][\"labels\"])\n",
    "        print(image1data['<OD>'][\"uids\"])\n",
    "        print(image2data['<OD>'][\"uids\"])\n",
    "        score = compare_images(image1data, image2data, \"<OD>\", \"labels\" ,\"uids\")\n",
    "        print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dd39c759-bee0-421b-baf4-aa334327f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f204d2c7-2352-45fa-9993-4e42e8b24714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'person', 'score': 0.9991, 'box': [926.0, 285.0, 1249.0, 794.0], 'width': 323.0, 'height': 509.0, 'area': 164407.0, 'area_ratio': 0.144521}\n",
      "{'label': 'person', 'score': 0.9988, 'box': [387.0, 306.0, 703.0, 793.0], 'width': 316.0, 'height': 487.0, 'area': 153892.0, 'area_ratio': 0.135278}\n",
      "{'label': 'person', 'score': 0.9946, 'box': [747.0, 331.0, 957.0, 797.0], 'width': 210.0, 'height': 466.0, 'area': 97860.0, 'area_ratio': 0.086023}\n",
      "{'label': 'person', 'score': 0.9951, 'box': [1237.0, 335.0, 1422.0, 797.0], 'width': 185.0, 'height': 462.0, 'area': 85470.0, 'area_ratio': 0.075132}\n",
      "{'label': 'person', 'score': 0.9858, 'box': [334.0, 297.0, 497.0, 795.0], 'width': 163.0, 'height': 498.0, 'area': 81174.0, 'area_ratio': 0.071355}\n",
      "{'label': 'person', 'score': 0.9971, 'box': [0.0, 302.0, 163.0, 796.0], 'width': 163.0, 'height': 494.0, 'area': 80522.0, 'area_ratio': 0.070782}\n",
      "{'label': 'person', 'score': 0.9815, 'box': [238.0, 296.0, 375.0, 795.0], 'width': 137.0, 'height': 499.0, 'area': 68363.0, 'area_ratio': 0.060094}\n",
      "{'label': 'person', 'score': 0.9894, 'box': [120.0, 343.0, 271.0, 794.0], 'width': 151.0, 'height': 451.0, 'area': 68101.0, 'area_ratio': 0.059864}\n",
      "{'label': 'car', 'score': 0.9857, 'box': [1219.0, 262.0, 1422.0, 444.0], 'width': 203.0, 'height': 182.0, 'area': 36946.0, 'area_ratio': 0.032477}\n",
      "{'label': 'car', 'score': 0.9697, 'box': [639.0, 203.0, 921.0, 326.0], 'width': 282.0, 'height': 123.0, 'area': 34686.0, 'area_ratio': 0.030491}\n",
      "{'label': 'person', 'score': 0.9404, 'box': [929.0, 270.0, 1055.0, 519.0], 'width': 126.0, 'height': 249.0, 'area': 31374.0, 'area_ratio': 0.027579}\n",
      "{'label': 'person', 'score': 0.9762, 'box': [567.0, 290.0, 693.0, 515.0], 'width': 126.0, 'height': 225.0, 'area': 28350.0, 'area_ratio': 0.024921}\n",
      "{'label': 'person', 'score': 0.9231, 'box': [1210.0, 297.0, 1332.0, 500.0], 'width': 122.0, 'height': 203.0, 'area': 24766.0, 'area_ratio': 0.02177}\n",
      "{'label': 'handbag', 'score': 0.9534, 'box': [1260.0, 557.0, 1385.0, 743.0], 'width': 125.0, 'height': 186.0, 'area': 23250.0, 'area_ratio': 0.020438}\n",
      "{'label': 'handbag', 'score': 0.9348, 'box': [456.0, 398.0, 601.0, 525.0], 'width': 145.0, 'height': 127.0, 'area': 18415.0, 'area_ratio': 0.016188}\n",
      "{'label': 'backpack', 'score': 0.9651, 'box': [1286.0, 417.0, 1421.0, 525.0], 'width': 135.0, 'height': 108.0, 'area': 14580.0, 'area_ratio': 0.012816}\n",
      "{'label': 'car', 'score': 0.9866, 'box': [1083.0, 215.0, 1196.0, 313.0], 'width': 113.0, 'height': 98.0, 'area': 11074.0, 'area_ratio': 0.009735}\n",
      "{'label': 'potted plant', 'score': 0.9497, 'box': [461.0, 77.0, 573.0, 168.0], 'width': 112.0, 'height': 91.0, 'area': 10192.0, 'area_ratio': 0.008959}\n",
      "{'label': 'handbag', 'score': 0.9536, 'box': [140.0, 692.0, 217.0, 800.0], 'width': 77.0, 'height': 108.0, 'area': 8316.0, 'area_ratio': 0.00731}\n",
      "{'label': 'truck', 'score': 0.9503, 'box': [1086.0, 147.0, 1166.0, 213.0], 'width': 80.0, 'height': 66.0, 'area': 5280.0, 'area_ratio': 0.004641}\n",
      "{'label': 'person', 'score': 0.9145, 'box': [493.0, 231.0, 550.0, 322.0], 'width': 57.0, 'height': 91.0, 'area': 5187.0, 'area_ratio': 0.00456}\n",
      "{'label': 'backpack', 'score': 0.947, 'box': [699.0, 391.0, 754.0, 463.0], 'width': 55.0, 'height': 72.0, 'area': 3960.0, 'area_ratio': 0.003481}\n",
      "{'label': 'cell phone', 'score': 0.9607, 'box': [197.0, 322.0, 214.0, 354.0], 'width': 17.0, 'height': 32.0, 'area': 544.0, 'area_ratio': 0.000478}\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "flipped = \"True\"\n",
    "\n",
    "pipeline = media_pipeline()\n",
    "pipeline.upload_image(Image.open(\"p1.JPG\"))\n",
    "pipeline.detect_objects()\n",
    "img_with_boxes = pipeline.draw_boxes(flipped)\n",
    "img_with_boxes.show()\n",
    "objects = pipeline.get_object_scores()\n",
    "\n",
    "for obj in objects:\n",
    "    print(obj)\n",
    "    \n",
    "# print(\"------------------------------\")\n",
    "\n",
    "\n",
    "# pipeline.upload_image(Image.open(\"p1.JPG\"))\n",
    "# pipeline.detect_objects()\n",
    "# img_with_boxes = pipeline.draw_boxes()\n",
    "# img_with_boxes.show()\n",
    "# objects = pipeline.get_object_scores()\n",
    "\n",
    "# for obj in objects:\n",
    "#     print(obj)\n",
    "\n",
    "# when comparing relative area_ratio we need use opposing area_ration * area for both images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f65067c7-5986-4243-b79d-054bfb4316e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def convert_mov_to_mp4(input_path, output_path=None):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"File not found: {input_path}\")\n",
    "    \n",
    "    if output_path is None:\n",
    "        base, _ = os.path.splitext(input_path)\n",
    "        output_path = base + \".mp4\"\n",
    "\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", input_path,\n",
    "        \"-vcodec\", \"libx264\",\n",
    "        \"-acodec\", \"aac\",\n",
    "        \"-strict\", \"experimental\",\n",
    "        \"-y\",  # Overwrite output file if it exists\n",
    "        output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Conversion successful: {output_path}\")\n",
    "        return output_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error during conversion:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "04d46e34-ebf5-4dcc-b115-f8d599654095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blur_score(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "004e68b6-a04f-49d2-8d26-f579bc71bb66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.101 / 61. 19.101\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x15b004850] Could not find codec parameters for stream 2 (Audio: none (apac / 0x63617061), 48000 Hz, 4 channels, 355 kb/s): unknown codec\n",
      "Consider increasing the value for the 'analyzeduration' (0) and 'probesize' (5000000) options\n",
      "[aist#0:2/none @ 0x15b00c530] Guessed Channel Layout: 4.0\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'IMG_6957.MOV':\n",
      "  Metadata:\n",
      "    major_brand     : qt  \n",
      "    minor_version   : 0\n",
      "    compatible_brands: qt  \n",
      "    creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "    com.apple.quicktime.location.accuracy.horizontal: 6.954517\n",
      "    com.apple.quicktime.full-frame-rate-playback-intent: 0\n",
      "    com.apple.quicktime.location.ISO6709: +41.9658-086.3612+205.924/\n",
      "    com.apple.quicktime.make: Apple\n",
      "    com.apple.quicktime.model: iPhone 16 Pro Max\n",
      "    com.apple.quicktime.software: 18.2\n",
      "    com.apple.quicktime.creationdate: 2025-04-19T18:31:27-0400\n",
      "  Duration: 00:00:09.60, start: 0.000000, bitrate: 46629 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 3840x2160, 46097 kb/s, 30 fps, 30 tbr, 600 tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Video\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : H.264\n",
      "      Side data:\n",
      "        displaymatrix: rotation of -90.00 degrees\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 124 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Audio\n",
      "        vendor_id       : [0][0][0][0]\n",
      "  Stream #0:2[0x3](und): Audio: none (apac / 0x63617061), 48000 Hz, 4.0, 355 kb/s\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Audio\n",
      "        vendor_id       : [0][0][0][0]\n",
      "  Stream #0:3[0x4](und): Data: none (mebx / 0x7862656D), 0 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Metadata\n",
      "  Stream #0:4[0x5](und): Data: none (mebx / 0x7862656D), 0 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Metadata\n",
      "  Stream #0:5[0x6](und): Data: none (mebx / 0x7862656D), 0 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Metadata\n",
      "  Stream #0:6[0x7](und): Data: none (mebx / 0x7862656D), 34 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Metadata\n",
      "  Stream #0:7[0x8](und): Data: none (mebx / 0x7862656D), 0 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Metadata\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
      "  Stream #0:1 -> #0:1 (aac (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x15b00d8a0] using cpu capabilities: ARMv8 NEON\n",
      "[libx264 @ 0x15b00d8a0] profile High, level 5.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x15b00d8a0] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'IMG_6957.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : qt  \n",
      "    minor_version   : 0\n",
      "    compatible_brands: qt  \n",
      "    com.apple.quicktime.creationdate: 2025-04-19T18:31:27-0400\n",
      "    com.apple.quicktime.location.accuracy.horizontal: 6.954517\n",
      "    com.apple.quicktime.full-frame-rate-playback-intent: 0\n",
      "    com.apple.quicktime.location.ISO6709: +41.9658-086.3612+205.924/\n",
      "    com.apple.quicktime.make: Apple\n",
      "    com.apple.quicktime.model: iPhone 16 Pro Max\n",
      "    com.apple.quicktime.software: 18.2\n",
      "    encoder         : Lavf61.7.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 2160x3840, q=2-31, 30 fps, 15360 tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Video\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.19.101 libx264\n",
      "      Side data:\n",
      "        cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-19T22:31:27.000000Z\n",
      "        handler_name    : Core Media Audio\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.19.101 aac\n",
      "frame=  268 fps= 20 q=29.0 size=   40192KiB time=N/A bitrate=N/A speed=N/A    its/s speed=0.64x     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful: IMG_6957.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[out#0/mp4 @ 0x60000091c000] video:42565KiB audio:150KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.027304%\n",
      "frame=  288 fps= 21 q=-1.0 Lsize=   42727KiB time=00:00:09.53 bitrate=36715.0kbits/s speed= 0.7x    \n",
      "[libx264 @ 0x15b00d8a0] frame I:2     Avg QP:23.77  size:1359746\n",
      "[libx264 @ 0x15b00d8a0] frame P:83    Avg QP:25.59  size:363326\n",
      "[libx264 @ 0x15b00d8a0] frame B:203   Avg QP:31.56  size: 52759\n",
      "[libx264 @ 0x15b00d8a0] consecutive B-frames:  0.7% 13.2%  8.3% 77.8%\n",
      "[libx264 @ 0x15b00d8a0] mb I  I16..4:  0.0% 39.6% 60.4%\n",
      "[libx264 @ 0x15b00d8a0] mb P  I16..4:  0.0%  0.5%  0.5%  P16..4: 40.8% 37.2% 18.7%  0.0%  0.0%    skip: 2.1%\n",
      "[libx264 @ 0x15b00d8a0] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8: 44.8%  8.7%  1.0%  direct: 3.1%  skip:42.5%  L0:61.4% L1:28.9% BI: 9.7%\n",
      "[libx264 @ 0x15b00d8a0] 8x8 transform intra:43.0% inter:64.8%\n",
      "[libx264 @ 0x15b00d8a0] coded y,uvDC,uvAC intra: 98.9% 56.9% 6.5% inter: 34.2% 6.8% 0.0%\n",
      "[libx264 @ 0x15b00d8a0] i16 v,h,dc,p: 50% 14%  9% 27%\n",
      "[libx264 @ 0x15b00d8a0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 19% 10%  8%  6% 13% 14% 11% 10%  9%\n",
      "[libx264 @ 0x15b00d8a0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 13%  9%  7% 12% 13% 11%  9%  8%\n",
      "[libx264 @ 0x15b00d8a0] i8c dc,h,v,p: 60% 12% 27%  1%\n",
      "[libx264 @ 0x15b00d8a0] Weighted P-Frames: Y:7.2% UV:1.2%\n",
      "[libx264 @ 0x15b00d8a0] ref P L0: 67.0% 20.0% 11.0%  1.9%  0.1%\n",
      "[libx264 @ 0x15b00d8a0] ref B L0: 98.5%  1.4%  0.1%\n",
      "[libx264 @ 0x15b00d8a0] ref B L1: 99.4%  0.6%\n",
      "[libx264 @ 0x15b00d8a0] kb/s:36321.31\n",
      "[aac @ 0x159e69350] Qavg: 9196.617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IMG_6957.mp4'"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_mov_to_mp4(\"IMG_6957.MOV\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
